# blog_rag_slackbot.py

"""
Step 1 ~ 7 í†µí•© êµ¬ì„±
- ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§
- ë§íˆ¬/ìŠ¤íƒ€ì¼ ë²¡í„°í™” (SentenceTransformer)
- Qdrant ë²¡í„° DB ì €ì¥
- FastAPI ìŠ¬ë™ ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´ ìˆ˜ì‹ 
- í”„ë¡¬í”„íŠ¸ ìƒì„±
- ìŠ¤íƒ€ì¼ ë°˜ì˜ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±
- ìŠ¬ë™ ì‘ë‹µ
"""

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from qdrant_client import QdrantClient
from langchain_qdrant import Qdrant
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from slack_sdk.webhook import WebhookClient
from bs4 import BeautifulSoup
import requests
import threading
import re
import os
import logging

# ---- ì „ì—­ ë¡œê¹… ì„¤ì • ----
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    filename="blog_slackbot.log",   # ë¡œê·¸ë¥¼ ì´ íŒŒì¼ë¡œ ì €ì¥
    filemode="a"                    # ê¸°ì¡´ íŒŒì¼ì— append (ë®ì–´ì“°ë ¤ë©´ "w")
)
logger = logging.getLogger(__name__)

# ---- ì„¤ì • ----
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
QDRANT_COLLECTION = "naver_blog"
NAVER_BLOG_ID = "sophia5460"

# ---- FastAPI ì•± ë° ë²¡í„° DB ì´ˆê¸°í™” ----
app = FastAPI()
qdrant_client = QdrantClient(host="localhost", port=6333)
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=QDRANT_COLLECTION,
    embeddings=embedding
)
retriever = vectorstore.as_retriever()
llm = OpenAI(temperature=0.7, max_tokens=2048)
slack = WebhookClient(SLACK_WEBHOOK_URL)

# ---- í…ìŠ¤íŠ¸ ì •ë¦¬ ----
def clean_text(text):
    text = re.sub(r"\n{2,}", "\n", text)
    text = re.sub(r"[ \t]{2,}", " ", text)
    return text.strip()

# ---- ë„¤ì´ë²„ ê²€ìƒ‰ í¬ë¡¤ë§ ----
def crawl_naver_place_summary(query: str) -> str:
    search_url = f"https://search.naver.com/search.naver?query={query}+ë§›ì§‘"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    summaries = []
    for li in soup.select("ul.lst_total._list li")[:3]:
        title = li.select_one("a.api_txt_lines")
        desc = li.select_one("div.total_dsc")
        if title and desc:
            summaries.append(f"{title.text.strip()} - {desc.text.strip()}")

    return "\n".join(summaries) if summaries else "ê´€ë ¨ ì‹ë‹¹ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# ---- í›„ê¸° ìƒì„± ----
def generate_post(request_text: str, past_style_docs: list[str]) -> str:
    style_snippets = "\n\n".join(past_style_docs[:2])
    prompt_template = PromptTemplate.from_template("""
        ë„ˆëŠ” ë¸”ë¡œê±°ì•¼. ì•„ë˜ëŠ” ë„ˆì˜ ê³¼ê±° ë¸”ë¡œê·¸ ê¸€ ìŠ¤íƒ€ì¼ì´ì•¼:
        ---
        {style}
        ---

        ì´ì œ ë‹¤ìŒ ìš”ì²­ì— ë§ì¶° ê°™ì€ ìŠ¤íƒ€ì¼ë¡œ í›„ê¸°ë¥¼ ì‘ì„±í•´ì¤˜:
        '{request}'
    """)
    prompt = prompt_template.format(style=style_snippets, request=request_text)

    full_output = ""
    remaining_prompt = prompt

    for _ in range(3):
        response = llm(remaining_prompt)
        full_output += response

        if len(response.strip()) < 1800:
            break
        remaining_prompt = "ê³„ì† ì´ì–´ì„œ ì‘ì„±í•´ì¤˜.\n"

    return full_output.strip()

# ---- ì‚¬ìš©ì ì…ë ¥ íŒŒì‹± ----
def parse_user_text(user_text: str) -> tuple[str, str]:
    if "/" in user_text:
        place, tone = user_text.split("/", 1)
        return place.strip(), tone.strip()
    return user_text.strip(), ""

# ---- ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´ ì²˜ë¦¬ (/blog ì‹ë‹¹ì´ë¦„ / ë§íˆ¬)
@app.post("/slack/slash")
async def slack_slash_command(request: Request):
    form = await request.form()
    user_text = form.get("text")
    response_url = form.get("response_url")
    place, tone = parse_user_text(user_text)

    ack_text = {
        "response_type": "ephemeral",
        "text": "í›„ê¸°ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš” â³"
    }

    def generate_and_send():
        try:
            # ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼ ë¶ˆëŸ¬ì˜¤ê¸°
            docs = retriever.get_relevant_documents("ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼")
            past_texts = [doc.page_content for doc in docs]

            # ì‹¤ì‹œê°„ ì‹ë‹¹ ì •ë³´ í¬ë¡¤ë§
            crawled_info = crawl_naver_place_summary(place)
            combined_input = f"'{place}'ë¼ëŠ” í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ ì‹ë‹¹ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n{crawled_info}\n\nì´ë¥¼ ì°¸ê³ í•´ì„œ ë¸”ë¡œê·¸ í›„ê¸°ë¥¼ ì‘ì„±í•´ì¤˜. {tone}"
            logger.info("â–¶ ë¸”ë¡œê·¸ ì‘ì„± í”„ë¡¬í”„íŠ¸:\n%s", combined_input)

            # í›„ê¸° ìƒì„±
            result = generate_post(combined_input, past_texts)

            # ìŠ¬ë™ ë©”ì‹œì§€ ë¶„í•  ì „ì†¡
            MAX_LEN = 3900
            chunks = [result[i:i + MAX_LEN] for i in range(0, len(result), MAX_LEN)]

            for idx, chunk in enumerate(chunks):
                prefix = "ğŸ“ ë¸”ë¡œê·¸ í›„ê¸° ì´ˆì•ˆ:\n\n" if idx == 0 else ""
                requests.post(response_url, json={
                    "response_type": "in_channel",
                    "text": prefix + chunk
                })

        except Exception as e:
            logger.error("âŒ ì˜¤ë¥˜ ë°œìƒ: %s", e)
            requests.post(response_url, json={
                "response_type": "ephemeral",
                "text": f"í›„ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            })

    threading.Thread(target=generate_and_send).start()
    return JSONResponse(content=ack_text)
